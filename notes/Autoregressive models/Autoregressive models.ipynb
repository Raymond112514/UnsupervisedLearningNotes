{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5091b25",
   "metadata": {},
   "source": [
    "# Generative models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f65432",
   "metadata": {},
   "source": [
    "Given $\\mathcal{D}=\\{X_i\\}_{i\\in [n]}\\sim p_{\\text{data}}(x)$, we want to learn the underlying data distribution $p_{\\text{data}}(x)$ using the training samples $X_1, X_2,..., X_n$. One way of doing this is to consider a family of parameterized distribution $p_{\\theta}(x)$ and then find the right parameter $\\theta$ such that \n",
    "\n",
    "$$p_{\\theta}(x)\\approx p_{\\text{data}}(x)$$\n",
    "\n",
    "One way of framing this mathematically is to find $\\theta$ that minimizes the KL-divergence between these two distributions\n",
    "\n",
    "$$\\text{KL}(p_{\\text{data}}||p_{\\theta}) = \\mathbb{E}_{p_{\\text{data}}}[-\\log p_{\\theta}(X)] - H(p_{\\text{data}})$$\n",
    "\n",
    "Note that we can drop the entropy term. Using sample average approximation, we have\n",
    "\n",
    "$$\\theta^* = \\underset{\\theta}{\\text{argmin}} \\frac{1}{n}\\sum_{i=1}^n -\\log p_{\\theta}(X_i)\\tag{1}$$\n",
    "\n",
    "Another interpretation of the optimization problem is given by the MLE estimation. We want to maximize the log-probability of likelihood\n",
    "\n",
    "$$\\theta^* = \\underset{\\theta}{\\text{argmax}} p_{\\theta}(X_1, X_2,..., X_n) = \\underset{\\theta}{\\text{argmax}} \\frac{1}{n}\\sum_{i=1}^n \\log p_{\\theta}(X_i)\\tag{2}$$\n",
    "\n",
    "Both formulations give us equivalent optimization problem. Now the question remains how to find a suitable class of parameterization $p_{\\theta}(x)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3419797d",
   "metadata": {},
   "source": [
    "## Autoregressive models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4347e67d",
   "metadata": {},
   "source": [
    "Assuming that the data $X$ is $K$ dimensional, say $X=[x_1, x_2,..., x_K]$, then the parameterized distribution can be written as $p_{\\theta}(x_1, x_2,..., x_K)$. Modeling this distribution directly is hard, especially because of low coverage of data points in high dimensional (curse of dimensionality). We need to somehow decompose the distribution. We can use the idea of Bayes net to decompose the distribution\n",
    "\n",
    "$$p_{\\theta}(x_1, x_2,..., x_K) = \\prod_i p(x_i|\\text{Parent}(x_i))$$\n",
    "\n",
    "However, it's not obvious how to choose the parent nodes. Therefore, a safe option is to consider the fully expressive Bayes net structure\n",
    "\n",
    "$$p_{\\theta}(x_1, x_2,..., x_K) = \\prod_i p(x_i|x_{<i})$$\n",
    "\n",
    "Where $x_{<i}$ denote the nodes $x_1, x_2,..., x_{i-1}$. Modelling $p_{\\theta}$ under such form is called an autoregressive model, since the value $x_i$ depends on $x_1, x_2,..., x_{i-1}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d6d11c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22fad14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1b772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0018d6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff3e563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
