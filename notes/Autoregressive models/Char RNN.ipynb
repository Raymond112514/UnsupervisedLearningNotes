{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c0432ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e73249",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "192cbcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/51/mms6ntkn5vg1pnv9_9003dwm0000gn/T/ipykernel_82617/1979571788.py:1: DtypeWarning: Columns (14,15,31,32) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"../Autoregressive models/nlp_comments/CommentsJan2017.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 231449\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../Autoregressive models/nlp_comments/CommentsJan2017.csv\")\n",
    "data = data[\"commentBody\"].values\n",
    "print(f\"Number of training samples: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae57eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace('<br/>', ' ')\n",
    "    text = text.replace('&amp', '')\n",
    "    text = text.replace(\"\\\"\", '')\n",
    "    text = \"\".join(v for v in text if v not in string.punctuation).lower()\n",
    "    text = text.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b53451fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.vectorize(clean_text)(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c10166",
   "metadata": {},
   "source": [
    "## Vocab class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8382f2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "START = 1\n",
    "END = 2\n",
    "UNK = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f62e94ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, text_array, freq=3):\n",
    "        self.text_array = text_array\n",
    "        self.ttov = {\"\": PAD, \"\": START,\n",
    "                     \"\": END, \"\": UNK}\n",
    "        self.vtot = dict((idx, token) for token, idx in self.ttov.items())\n",
    "        self.length = 4\n",
    "        self.counter = Counter()\n",
    "        self.freq = freq\n",
    "        self.build_vocab()\n",
    "\n",
    "    def add(self, token):\n",
    "        self.ttov[token] = self.length\n",
    "        self.vtot[self.length] = token\n",
    "        self.length += 1\n",
    "\n",
    "    def build_vocab(self):\n",
    "        for sentence in self.text_array:\n",
    "            for token in sentence.split():\n",
    "                self.counter.update([token])\n",
    "\n",
    "        for token, counter in self.counter.items():\n",
    "            if counter >= self.freq:\n",
    "                self.add(token)\n",
    "\n",
    "    def vecToText(self, vec):\n",
    "        text = []\n",
    "        for v in vec:\n",
    "            if v in self.vtot:\n",
    "                text.append(self.vtot[v])\n",
    "            else:\n",
    "                text.append(\"\")\n",
    "        return text\n",
    "\n",
    "    def textToVec(self, text):\n",
    "        vec = []\n",
    "        for t in text:\n",
    "            if t in self.ttov:\n",
    "                vec.append(self.ttov[t])\n",
    "            else:\n",
    "                vec.append(UNK)\n",
    "        return torch.tensor(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6fc67885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dictionary: 55446\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(data)\n",
    "print(f\"Length of dictionary: {vocab.length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5febf308",
   "metadata": {},
   "source": [
    "## Dataset/Dataloader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39112eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, vocab):\n",
    "        self.text = text\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]\n",
    "        return self.vocab.textToVec(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "87736aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    text = [data for data in batch]\n",
    "    max_len = max([len(data) for data in batch]) + 1\n",
    "    text_in = [torch.cat([torch.tensor([START]), txt]) for txt in text]\n",
    "    text_out = [torch.cat([txt, torch.tensor([END])]) for txt in text]\n",
    "    return pad(text_in, max_len), pad(text_out, max_len)\n",
    "\n",
    "def pad(texts, max_len):\n",
    "    padded = []\n",
    "    for text in texts:\n",
    "        while len(text) < max_len:\n",
    "            text = torch.cat([text, torch.tensor([PAD])])\n",
    "        padded.append(text)\n",
    "    return torch.stack(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bafd40",
   "metadata": {},
   "source": [
    "## CharRNN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "18d37625",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        param vocab_size (V): number of vocab\n",
    "        param embedding_dim (E): number of embedding dimension\n",
    "        param hidden_dim (H): number of hidden dimension\n",
    "        \n",
    "        length (L): length of sentence\n",
    "        batch (B): batch size\n",
    "        \"\"\"\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True, dropout=0.2)\n",
    "        self.linear = torch.nn.Linear(in_features=hidden_dim, out_features=vocab_size)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Input shape: [B, L]\n",
    "        Embedding: [B, L, E]\n",
    "        RNN: [B, L, H]\n",
    "        Out: [B, L, V]\n",
    "        \"\"\"\n",
    "        X = self.embedding(X)\n",
    "        X, _ = self.rnn(X)\n",
    "        out = self.linear(X)\n",
    "        return out\n",
    "    \n",
    "    def generate(self, max_length=100):\n",
    "        \"\"\"\n",
    "        sentence: stores a collection of characters\n",
    "        character: store the index (integer) of current character\n",
    "        \"\"\"\n",
    "        sentence = [START]\n",
    "        character = START\n",
    "        hidden = None\n",
    "        with torch.no_grad():\n",
    "            while len(sentence) <= max_length:\n",
    "                X = torch.tensor([[character]], dtype=torch.long)\n",
    "                X = self.embedding(X)\n",
    "                X, hidden = self.rnn(X, hidden)\n",
    "                X = self.linear(X)\n",
    "                character = torch.argmax(X, axis=-1).unsqueeze(0).unsqueeze(0).item()\n",
    "                sentence.append(character)\n",
    "                if character == END:\n",
    "                    break\n",
    "        return torch.tensor(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "348d5a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed forward test!\n",
      "Passed generate test!\n"
     ]
    }
   ],
   "source": [
    "# Test CharRNN class\n",
    "batch_size = 64\n",
    "length = 100\n",
    "vocab_size = 100\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "charrnn = CharRNN(vocab_size, embedding_dim, hidden_dim)\n",
    "X = torch.randint(high=vocab_size, size=(batch_size, length))\n",
    "out = charrnn(X)\n",
    "\n",
    "# Forward test\n",
    "if out.shape == (batch_size, length, vocab_size):\n",
    "    print(f\"Passed forward test!\")\n",
    "else:\n",
    "    print(f\"Test failed, the outputted shape is {out.shape}\")\n",
    "    \n",
    "# Generate test\n",
    "sample = charrnn.generate()\n",
    "if len(sample.shape) == 1:\n",
    "    print(f\"Passed generate test!\")\n",
    "else:\n",
    "    print(f\"Test failed, the sample setence shape is {sample.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce5bc3f",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bac504d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, n_epochs, lr=1e-3, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD, reduction='mean')\n",
    "    for epoch in range(n_epochs):\n",
    "        average_loss = 0\n",
    "        n = 0\n",
    "        for X, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred.view(-1, vocab.length), y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            average_loss += loss.item()\n",
    "            n += 1\n",
    "        print(f\"Epoch {epoch + 1} average loss: {average_loss / n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fda007a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9168bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset(data[:64], vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7b292039",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNN(vocab.length, 128, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0bef22d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average loss: 3.2414730489254\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[47], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, n_epochs, lr, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_pred\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab\u001b[38;5;241m.\u001b[39mlength), y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m average_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, dataloader, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5874e8c3",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e7f0a4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b0d351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
