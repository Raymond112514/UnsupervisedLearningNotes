{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7f2edde",
   "metadata": {},
   "source": [
    "# Diffusion models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017c19ab",
   "metadata": {},
   "source": [
    "In diffusion model, we have a forward process $q(\\mathbf{x}_t|\\mathbf{x}_{t-1})$ which adds noise according to some variance schedule $\\{\\beta_t\\}\\in (0,1)$ (where $\\mathbf{x}_0$ denote the original image). Formally, define\n",
    "\n",
    "$$q(\\mathbf{x}_t|\\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t|\\sqrt{1-\\beta_t}\\mathbf{x}_{t-1}, \\beta_t \\mathbf{I})$$\n",
    "\n",
    "Letting $\\alpha_t=1-\\beta_t$ and $\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i$, we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbf{x}_t&= \\sqrt{\\alpha_t}\\mathbf{x}_{t-1} + \\sqrt{1-\\alpha_t} \\epsilon_{t-1}\\\\\n",
    "    &= \\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t-1}}\\mathbf{x}_{t-2} + \\sqrt{1-\\alpha_{t-1}}\\epsilon_{t-2}) + \\sqrt{1-\\alpha_t}\\epsilon_{t-1}\\\\\n",
    "    &= \\sqrt{\\alpha_t\\alpha_{t-1}}\\mathbf{x}_{t-2} + \\sqrt{1-\\alpha_t\\alpha_{t-1}}\\epsilon_{t-3}\\\\\n",
    "    &= \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_{0} + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since $\\alpha_t\\in (0,1)$, we see that $\\bar{\\alpha}_t\\rightarrow 0$ as $t\\to\\infty$. This suggests that as $t\\to\\infty$, we have that\n",
    "\n",
    "$$q(\\mathbf{x}_t|\\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t|\\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_{0}, (1-\\bar{\\alpha}_t)\\mathbf{I})\\rightarrow \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$$\n",
    "\n",
    "In other words, the distribution becomes an isotropic Gaussian. If we can reverse the above process, then we can generate new samples by sampling from $\\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ and then sample from $q(\\mathbf{x}_{t-1}|\\mathbf{x}_t)$. Unfortunately, we can not easily estimate $q(\\mathbf{x}_{t-1}|\\mathbf{x}_t)$. Therefore, consider a parameterized estimation of these distributions $p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)$. Note that this is essentially a latent variable model with latent variables $\\mathbf{z}=\\mathbf{x}_{1:T}$. Therefore, applying the variational lower bound, we have \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\log p(\\mathbf{x}_0) &\\geq \\mathbb{E}_{\\mathbf{x}_{1:T}|\\mathbf{x}_0} [\\log p(\\mathbf{x}_0|\\mathbf{x}_{1:T})] - \\mathcal{D}_{KL}(q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)||p(\\mathbf{x}_{1:T}))\\\\\n",
    "    &= \\mathbb{E}_{\\mathbf{x}_{1:T}|\\mathbf{x}_0} \\bigg[\\log \\frac{p(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)}\\bigg]\\\\\n",
    "    &= \\mathbb{E}_{\\mathbf{x}_{1:T}|\\mathbf{x}_0} \\bigg[\\log \\frac{p(\\mathbf{x}_T)p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_1)\\prod_{t=1}^{T-1}p_{\\theta}(\\mathbf{x}_{t}|\\mathbf{x}_{t+1})}{q(\\mathbf{x}_T|\\mathbf{x}_{T-1})\\prod_{t=1}^{T-1} q(\\mathbf{x}_t|\\mathbf{x}_{t-1})}\\bigg]\\\\\n",
    "    &= \\mathbb{E}_{\\mathbf{x}_{1:T}|\\mathbf{x}_0}[p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_1)] + \\mathbb{E}_{\\mathbf{x}_{1:T}|\\mathbf{x}_0} \\bigg[\\log\\frac{p(\\mathbf{x}_T)}{q(\\mathbf{x}_T|\\mathbf{x}_{T-1})}\\bigg] + \\sum_{i=1}^{T-1} \\mathbb{E}_{\\mathbf{x}_{1:T}|\\mathbf{x}_0} \\bigg[\\frac{p_{\\theta}(\\mathbf{x}_{t}|\\mathbf{x}_{t+1})}{q(\\mathbf{x}_t|\\mathbf{x}_{t-1})}\\bigg]\\\\\n",
    "    &= \\mathbb{E}_{\\mathbf{x}_{1}|\\mathbf{x}_0}[p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_1)] + \\mathbb{E}_{\\mathbf{x}_{T-1}, \\mathbf{x}_T|\\mathbf{x}_0} \\bigg[\\log\\frac{p(\\mathbf{x}_T)}{q(\\mathbf{x}_T|\\mathbf{x}_{T-1})}\\bigg] + \\sum_{i=1}^{T-1} \\mathbb{E}_{\\mathbf{x}_{t-1}, \\mathbf{x}_{t}, \\mathbf{x}_{t+1}|\\mathbf{x}_{0}} \\bigg[\\frac{p_{\\theta}(\\mathbf{x}_{t}|\\mathbf{x}_{t+1})}{q(\\mathbf{x}_t|\\mathbf{x}_{t-1})}\\bigg]\\\\\n",
    "    &= \\underbrace{\\mathbb{E}_{\\mathbf{x}_{1}|\\mathbf{x}_0}[p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_1)]}_{\\text{reconstruction term}} + \n",
    "    \\underbrace{\\mathbb{E}_{\\mathbf{x}_{T-1}|\\mathbf{x}_0} [\\mathcal{D}_{KL}(q(\\mathbf{x}_T|\\mathbf{x}_{T-1})||p(\\mathbf{x}_T))]}_{\\text{prior matching term}}\n",
    "    + \\sum_{i=1}^{T-1} \\underbrace{\\mathbb{E}_{\\mathbf{x}_{t-1}, \\mathbf{x}_{t+1}|\\mathbf{x}_{0}} [\\mathcal{D}_{KL}(q(\\mathbf{x}_t|\\mathbf{x}_{t-1})||p_{\\theta}(\\mathbf{x}_{t}|\\mathbf{x}_{t+1}))]}_{\\text{consistency term}}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Like the case for variational autoencoder, the variational lower bound consists of three terms\n",
    "\n",
    "1. Reconstruction term:  measures the likelihood of reconstruction in the first latent layer.\n",
    "2. Prior matching term: ensures that the learned final latent distribution matches the prior distribution.\n",
    "3. Consistency term: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ca51ba",
   "metadata": {},
   "source": [
    "## A simplified training scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3c72b5",
   "metadata": {},
   "source": [
    "Note that all of the terms in the variational lower bound can be computed using Monte Carlo estimates. However, in the consistency term, we need to sample over two variables, which has higher variance than having only one variable. Therefore, it is more desirable to reformulate ELBO so that each conditionals only condition on one variable. This can be done by noting that\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd33a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523b4d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffeb1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4147ee76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7562ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b433a99d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42ace5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ae142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13d3dc9b",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "     \\mathbb{E}_{\\mathbf{x}_{1:T}|\\mathbf{x}_0} \\bigg[\\log \\frac{p(\\mathbf{x}_{1:T}|\\mathbf{x}_0)}{p(\\mathbf{x}_{0:T})}\\bigg] &=  \\mathbb{E}_{\\mathbf{x}_{1:T}|\\mathbf{x}_0} \\bigg[\\log\\frac{\\prod_{t=1}^T q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})}{p(\\mathbf{x}_T)\\prod_{t=1}^T p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}\\bigg]\\\\\n",
    "     &= \\mathbb{E}_{\\mathbf{x}_{1:T}|\\mathbf{x}_0} \\bigg[-\\log p(\\mathbf{x}_T) + \\sum_{t=1}^T \\log \\frac{ q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})}{p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}\\bigg]\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We can further simplify the work by noting that. This gives us\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "     \\log p(\\mathbf{x}_0) &\\geq \\mathbb{E}_{\\mathbf{x}_{1:T}|\\mathbf{x}_0} \\bigg[-\\log p(\\mathbf{x}_T) + \\sum_{t=1}^T \\log \\frac{ q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})}{p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}\\bigg]\\\\\n",
    "     &= \\mathbb{E}_{\\mathbf{x}_{1:T}|\\mathbf{x}_0} \\bigg[-\\log p(\\mathbf{x}_T) + \\sum_{t=1}^T \\log \\bigg(\\frac{ q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_0)}{p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}\\cdot \\frac{q(\\mathbf{x}_{t}|\\mathbf{x}_0)}{q(\\mathbf{x}_{t-1}|\\mathbf{x}_0)}\\bigg)+\\log \\frac{q(\\mathbf{x}_1|\\mathbf{x}_0}{p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_1)}\\bigg]\\\\\n",
    "     &= \\mathbb{E}_{\\mathbf{x}_{1:T}|\\mathbf{x}_0} \\bigg[-\\log p(\\mathbf{x}_T) + \\sum_{t=1}^T \\log \\frac{ q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_0)}{p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}+\\sum_{t=1}^T \\log\\frac{q(\\mathbf{x}_{t}|\\mathbf{x}_0)}{q(\\mathbf{x}_{t-1}|\\mathbf{x}_0)}+\\log \\frac{q(\\mathbf{x}_1|\\mathbf{x}_0)}{p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_1)}\\bigg]\\\\\n",
    "     &= \\mathbb{E}_{\\mathbf{x}_{1:T}|\\mathbf{x}_0} \\bigg[-\\log p(\\mathbf{x}_T) + \\sum_{t=1}^T \\log \\frac{ q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_0)}{p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}+\\sum_{t=1}^T \\log\\frac{q(\\mathbf{x}_{t}|\\mathbf{x}_0)}{q(\\mathbf{x}_{t-1}|\\mathbf{x}_0)}+\\log \\frac{q(\\mathbf{x}_1|\\mathbf{x}_0)}{p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_1)}\\bigg]\\\\\n",
    "     &= \\mathbb{E}_{\\mathbf{x}_{1:T}|\\mathbf{x}_0} \\bigg[\\log\\frac{q(\\mathbf{x}_T|\\mathbf{x}_0)}{p(\\mathbf{x}_T)} + \\sum_{t=1}^T \\log \\frac{ q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_0)}{p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}-\\log p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_1)\\bigg]\\\\\n",
    "     &= \\mathbb{E}_{\\mathbf{x}_{T}|\\mathbf{x}_0} \\bigg[\\log\\frac{q(\\mathbf{x}_T|\\mathbf{x}_0)}{p(\\mathbf{x}_T)}\\bigg] + \\sum_{t=2}^T \\mathbb{E}_{\\mathbf{x}_{t-1}, \\mathbf{x}_{t}|\\mathbf{x}_0}\\bigg[\\log \\frac{ q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_0)}{p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}\\bigg]- \\mathbb{E}_{\\mathbf{x}_{1}|\\mathbf{x}_0}\\bigg[\\log p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_1)\\bigg]\\\\\n",
    "     &= \\text{KL}(q(\\mathbf{x}_T|\\mathbf{x}_0)||p(\\mathbf{x}_T)) + \\sum_{t=2}^T \\mathbb{E}_{ \\mathbf{x}_{t}|\\mathbf{x}_0}[\\text{KL}(q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_0)||p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t))] - \\mathbb{E}_{\\mathbf{x}_{1}|\\mathbf{x}_0}[\\log p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_1)]\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Each of these term can be computed efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcab1109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
