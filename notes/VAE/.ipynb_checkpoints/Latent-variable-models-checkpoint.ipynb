{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb1cc2ee",
   "metadata": {},
   "source": [
    "# Latent variable models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b0ed2a",
   "metadata": {},
   "source": [
    "In latent variable models, we assume that the data distribution $\\mathbf{x}$ is dependent on some unobserved variables called latent variables $\\mathbf{z}$. \n",
    "\n",
    "```{image} Latent-variable-model.png\n",
    ":alt: fishy\n",
    ":class: bg-primary mb-1\n",
    ":width: 300px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "<p style=\"text-align: center; font-size: 14px;\"> \n",
    "    <strong> Figure 1 </strong>: Bayes net structure of latent variable model\n",
    "</p>\n",
    "\n",
    "Assume that we have the prior distribution $p(\\mathbf{z})$ and the parameterized conditional distribution $p_{\\theta}(\\mathbf{x}|\\mathbf{z})$, the log-likelihood objective now becomes\n",
    "\n",
    "$$\\log p(x) = \\log \\int p_{\\theta}(\\mathbf{x}|\\mathbf{z})p(\\mathbf{z})d\\mathbf{z} = \\mathbb{E}_{\\mathbf{z}}[p_{\\theta}(\\mathbf{x}|\\mathbf{z})]$$\n",
    "\n",
    "One way of training the objective is through Monte Carlo sampling. Let $\\{\\mathbf{z}_i\\}\\sim p_{\\mathbf{z}}(z)$, we can estimate the expectation as\n",
    "\n",
    "$$\\log p(x) \\approx \\frac{1}{K}\\sum_{i=1}^K p_{\\theta}(\\mathbf{x}|\\mathbf{z}_i)$$\n",
    "\n",
    "However, this method has poor coverage properties. A more efficient way is to consider importance sampling. Let $q(\\mathbf{z})$ be any distribution, note that we can rewrite the integral as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\log p(\\mathbf{x}) &= \\log \\int p_{\\theta}(\\mathbf{x},\\mathbf{z})p(\\mathbf{z})d\\mathbf{z}\\\\\n",
    "    &=\\log \\int \\frac{p_{\\theta}(\\mathbf{x},\\mathbf{z})}{q(\\mathbf{z})}q(\\mathbf{z})d\\mathbf{z}\\\\\n",
    "    &= \\log \\mathbb{E}_{\\mathbf{z}\\sim q(\\mathbf{z})} \\bigg[\\frac{p_{\\theta}(\\mathbf{x},\\mathbf{z})}{q(\\mathbf{z})}\\bigg]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "By Jensen's inequality, we can move the logarithm inside expectation, this gives us a lower bound on the objective. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\log p(\\mathbf{x})\n",
    "    &= \\log \\mathbb{E}_{\\mathbf{z}\\sim q(\\mathbf{z})} \\bigg[\\frac{p_{\\theta}(\\mathbf{x},\\mathbf{z})}{q(\\mathbf{z})}\\bigg]\\\\\n",
    "    &\\geq \\mathbb{E}_{\\mathbf{z}\\sim q(\\mathbf{z})} \\bigg[\\log \\frac{p_{\\theta}(\\mathbf{x},\\mathbf{z})}{q(\\mathbf{z})}\\bigg]\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "How should we choose the distribution $q(\\mathbf{z}$. Ideally, we want the lower bound to be as tight as possible. The following theorem shows that this is acheived when $q(\\mathbf{z})$ is the posterior distribution $p(\\mathbf{z}|\\mathbf{x})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb1394b",
   "metadata": {},
   "source": [
    "````{prf:theorem} ELBO\n",
    ":label: my-theorem \n",
    "\n",
    "The lower bound of \n",
    "\n",
    "$$\\log p(\\mathbf{x})\\geq \\mathbb{E}_{\\mathbf{z}\\sim q(\\mathbf{z})} \\bigg[\\log \\frac{p_{\\theta}(\\mathbf{x},\\mathbf{z})}{q(\\mathbf{z})}\\bigg]$$\n",
    "\n",
    "Is attained when $q(\\mathbf{z}) = p(\\mathbf{z}|\\mathbf{x})$\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d9f267",
   "metadata": {},
   "source": [
    "````{prf:proof}\n",
    "Recall that Jensen's inquality holds when the random variable is constant almost everywhere. This means that when \n",
    "\n",
    "$$q(\\mathbf{z}) \\propto p_{\\theta}(\\mathbf{x},\\mathbf{z}) \\propto p(\\mathbf{z}|\\mathbf{x})$$\n",
    "\n",
    "Equality holds. Below we provide another proof using KL-divergence. \n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bb118e",
   "metadata": {},
   "source": [
    "Theorem 1 suggests that we should choose $q(\\mathbf{z}) = p(\\mathbf{z}|\\mathbf{x})$. However, the posterior is usually hard to compute because it involves estimating an integral. \n",
    "\n",
    "$$p(\\mathbf{z}|\\mathbf{x}) = \\frac{p(\\mathbf{x}|\\mathbf{z})p(\\mathbf{z})}{\\int p(\\mathbf{x}|\\mathbf{z})p(\\mathbf{z})dz}$$\n",
    "\n",
    "Therefore, we instead approximate $p(\\mathbf{z}|\\mathbf{x})$ using a family of parameterized distribution $q_{\\phi}(\\mathbf{z}|\\mathbf{x}) \\in \\mathcal{Q}$. This approximation introduces a gap between the variational lower bound and the true objective, as the lower bound is only tight when $q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ exactly matches $p(\\mathbf{z}|\\mathbf{x})$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\log p(\\mathbf{x}) &= \\mathbb{E}_{\\mathbf{z}\\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})}[\\log p(\\mathbf{x})] \\\\\n",
    "    &= \\mathbb{E}_{\\mathbf{z}\\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\bigg[\\log\\frac{p_{\\theta}(\\mathbf{x},\\mathbf{z})}{p(\\mathbf{z}|\\mathbf{x})}\\bigg]\\\\\n",
    "    &= \\mathbb{E}_{\\mathbf{z}\\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\bigg[\\log\\frac{p_{\\theta}(\\mathbf{x},\\mathbf{z})}{p(\\mathbf{z}|\\mathbf{x})}\\cdot \\frac{q_{\\phi}(\\mathbf{z}|\\mathbf{x})}{q_{\\phi}(\\mathbf{z}|\\mathbf{x})}\\bigg]\\\\\n",
    "    &= \\mathbb{E}_{\\mathbf{z}\\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\bigg[\\log\\frac{p_{\\theta}(\\mathbf{x},\\mathbf{z})}{q_{\\phi}(\\mathbf{z}|\\mathbf{x})}\\bigg]+ \\mathbb{E}_{\\mathbf{z}\\sim p(\\mathbf{z}|\\mathbf{x})}\\bigg[\\log\\frac{q_{\\phi}(\\mathbf{z}|\\mathbf{x})}{p(\\mathbf{z}|\\mathbf{x})}\\bigg]\\\\\n",
    "    &= \\mathbb{E}_{\\mathbf{z}\\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\bigg[\\log\\frac{p_{\\theta}(\\mathbf{x},\\mathbf{z})}{q_{\\phi}(\\mathbf{z}|\\mathbf{x})}\\bigg] - \\text{KL}(q_{\\phi}(\\mathbf{z}|\\mathbf{x})||p(\\mathbf{z}|\\mathbf{x}))\\\\\n",
    "    &\\geq \\mathbb{E}_{\\mathbf{z}\\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\bigg[\\log\\frac{p_{\\theta}(\\mathbf{x},\\mathbf{z})}{q_{\\phi}(\\mathbf{z}|\\mathbf{x})}\\bigg]\n",
    "\\end{align*}$$\n",
    "\n",
    "The derivation above provides another proof for Theorem 1. It explicitly demonstrates that the term is a lower bound and becomes tighter as $q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ approximates $p(\\mathbf{z}|\\mathbf{x})$ more closely, measured by the KL-divergence. The lower bound is known as the Evidence Lower Bound (ELBO), and we use it as a proxy for the true log-likelihood. Instead of maximizing the log-likelihood directly, we optimize over \n",
    "$\\theta$ and $\\phi$ to make ELBO as large as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38078429",
   "metadata": {},
   "source": [
    "## Variational autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a2c53e",
   "metadata": {},
   "source": [
    "We can further decompose ELBO as follows\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{ELBO} &= \\mathbb{E}_{\\mathbf{z}\\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\bigg[\\log\\frac{p_{\\theta}(\\mathbf{x},\\mathbf{z})}{q_{\\phi}(\\mathbf{z}|\\mathbf{x})}\\bigg]\\\\\n",
    "    &=  \\mathbb{E}_{\\mathbf{z}\\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} \\bigg[\\log\\frac{p_{\\theta}(\\mathbf{x}|\\mathbf{z})p(\\mathbf{z})}{q_{\\phi}(\\mathbf{z}|\\mathbf{x})}\\bigg]\\\\\n",
    "    &=  \\underbrace{\\mathbb{E}_{\\mathbf{z}\\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})} [\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})]}_{\\text{Reconstruction term}} -\n",
    "    \\underbrace{\\mathcal{D}_{KL}(q_{\\phi}(\\mathbf{z}|\\mathbf{x})||p(\\mathbf{z}))}_{\\text{Prior matching term}}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The two terms in ELBO has intuitive interpretations.\n",
    "\n",
    "1. Reconstruction term: If we treat $q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ as an encoder and $p_{\\theta}(\\mathbf{x}|\\mathbf{z})$ as a decoder, then the first term measures the likelihood of reconstruction (i.e, mapping from $\\mathbf{x}$ to $\\mathbf{z}$ and then back to $\\mathbf{x}$).\n",
    "2. Prior matching term: The second term ensures that the estimated posterior is similar to our prior belief over the latent variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10feab0d",
   "metadata": {},
   "source": [
    "## Reparameterization trick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd81a2",
   "metadata": {},
   "source": [
    "Now it remains to thing about how to parameterize the distributions. The decoder $p_{\\theta}(\\mathbf{x}|\\mathbf{z})$ is usually parameterized by some neural network, where as the encoder $q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ and the prior $p(\\mathbf{z})$ is commonly modeled as a multivariate Gaussian\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    q_{\\phi}(\\mathbf{z}|\\mathbf{x}) &= \\mathcal{N}(\\mathbf{z}; \\mathbf{\\mu}_{\\phi}(\\mathbf{x}), \\sigma^2_{\\phi}(\\mathbf{x})\\mathbf{I})\\\\\n",
    "    p(\\mathbf{z}) &= \\mathcal{N}(\\mathbf{z}; \\mathbf{0}, \\mathbf{I})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can now estimate ELBO as follows\n",
    "\n",
    "$$\\text{ELBO}\\approx \\frac{1}{n}\\sum_{i=1}^n \\log p_{\\theta}(\\mathbf{x}|\\mathbf{z}_i)-\n",
    "    \\mathcal{D}_{KL}(q_{\\phi}(\\mathbf{z}|\\mathbf{x})||p(\\mathbf{z}))\\hspace{10mm}\\mathbf{z}_i\\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})$$\n",
    "    \n",
    "Where we used Monte Carlo estimate for the reconstruction term. One problem with the above objective is that we cannot compute the gradient with respect to $\\phi$ since we only have samples $\\mathbf{z}_i$ that is generated from $q_{\\phi}(\\mathbf{z}|\\mathbf{x})$. One way of solving this issue is through the reparameterization trick, which uses the fact that\n",
    "\n",
    "$$\\mathbf{z}\\sim \\mathcal{N}(\\mathbf{z}; \\mathbf{\\mu}_{\\phi}(\\mathbf{x}), \\sigma^2_{\\phi}(\\mathbf{x})\\mathbf{I})\\hspace{3mm}\\Longleftrightarrow \\hspace{3mm} \\mathbf{z} = \\mathbf{\\mu}_{\\phi}(\\mathbf{x}) + \\sigma^2_{\\phi} \\odot \\mathbf{\\epsilon}\\hspace{10mm} \\epsilon\\sim \\mathcal{N}(\\mathbf{z}; \\mathbf{0}, \\mathbf{I})$$\n",
    "\n",
    "Therefore, we can re-express our objective as\n",
    "\n",
    "$$\\text{ELBO}\\approx \\frac{1}{n}\\sum_{i=1}^n \\log p_{\\theta}(\\mathbf{x}|\\mathbf{\\mu}_{\\phi}(\\mathbf{x}) + \\sigma^2_{\\phi} \\odot \\mathbf{\\epsilon}_i)-\n",
    "    \\mathcal{D}_{KL}(q_{\\phi}(\\mathbf{z}|\\mathbf{x})||p(\\mathbf{z}))\\hspace{10mm}\\epsilon_i\\sim \\mathcal{N}(\\mathbf{z}; \\mathbf{0}, \\mathbf{I})$$\n",
    "    \n",
    "Which is now differentiable over both $\\theta$ and $\\phi$. The overall architecture of a variational autoencoder is shown in Figure 2.\n",
    "\n",
    "```{image} VAE-architecture.png\n",
    ":alt: fishy\n",
    ":class: bg-primary mb-1\n",
    ":width: 500px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "<p style=\"text-align: center; font-size: 14px;\"> \n",
    "    <strong> Figure 2 </strong>: A VAE consists of an encoder $q_{\\phi}(\\mathbf{z}|\\mathbf{x})$ which maps inputs to the latent space. The decoder $p_{\\theta}(\\mathbf{x}|\\mathbf{z})$ then attempts to reconstruct the input. \n",
    "</p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
